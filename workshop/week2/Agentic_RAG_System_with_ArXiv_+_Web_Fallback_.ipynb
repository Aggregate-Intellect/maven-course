{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3sQoe2Tg_fb"
      },
      "source": [
        "#  Agentic RAG System with ArXiv + Web Fallback\n",
        "\n",
        "This project implements an **intelligent research assistant** that retrieves and synthesizes information using:\n",
        "1. **ArXiv papers** as the primary knowledge source (**RAG approach**)\n",
        "2. **Web search (Tavily API)** as a fallback mechanism\n",
        "3. **LangGraph** for orchestrating the decision-making workflow\n",
        "\n",
        "##  Purpose\n",
        "\n",
        "The system is designed to provide high-quality, research-backed answers to technical and scientific questions by:\n",
        "- Prioritizing academic and research papers from ArXiv for scientific queries\n",
        "- Falling back to web search for recent developments or non-academic topics\n",
        "- Maintaining conversation context for coherent multi-turn interactions\n",
        "- Ensuring proper attribution and citations in responses\n",
        "\n",
        "##  Prerequisites\n",
        "\n",
        "To use this system, you'll need:\n",
        "\n",
        "1. **OpenAI API Key**\n",
        "   - Required for:\n",
        "     - Text embeddings (for semantic search)\n",
        "     - Response generation (GPT-4 Turbo)\n",
        "     - Routing decisions (GPT-3.5 Turbo)\n",
        "   - Get it from: [OpenAI Platform](https://platform.openai.com)\n",
        "\n",
        "2. **Tavily API Key**\n",
        "   - Required for:\n",
        "     - Web search fallback functionality\n",
        "     - Real-time information retrieval\n",
        "     - Academic domain filtering\n",
        "   - Get it from: [Tavily](https://app.tavily.com)\n",
        "\n",
        "3. **Python Environment**\n",
        "   - Python 3.8 or higher\n",
        "   - Required packages (will be installed automatically):\n",
        "     - langchain-community\n",
        "     - langchain_chroma\n",
        "     - langchain_core\n",
        "     - langchain_openai\n",
        "     - langchain_text_splitters\n",
        "     - langgraph\n",
        "     - tavily-python\n",
        "     - openai\n",
        "     - python-dotenv\n",
        "\n",
        "\n",
        "##  Agentic Workflow Architecture\n",
        "\n",
        "The user workflow is translated into an agentic system through the following components:\n",
        "\n",
        "1. **State Management**\n",
        "   - **Conversation State**: Tracks user queries, system responses, and context\n",
        "   - **Search State**: Maintains information about current search results and sources\n",
        "   - **Decision State**: Stores routing decisions and their rationale\n",
        "\n",
        "2. **Agent Components**\n",
        "   - **Router Agent**: Makes intelligent decisions about information sources\n",
        "     - Analyzes query type and context\n",
        "     - Determines optimal search strategy\n",
        "     - Handles fallback mechanisms\n",
        "   \n",
        "   - **Search Agent**: Executes information retrieval\n",
        "     - Manages ArXiv API interactions\n",
        "     - Handles Tavily web search\n",
        "     - Processes and filters results\n",
        "   \n",
        "   - **Synthesis Agent**: Combines and formats information\n",
        "     - Merges multiple sources\n",
        "     - Ensures proper attribution\n",
        "     - Generates coherent responses\n",
        "\n",
        "3. **Feedback Loop**\n",
        "   - System learns from user interactions\n",
        "   - Improves routing decisions over time\n",
        "   - Adapts to user preferences and query patterns\n",
        "\n",
        "##  Data Requirements and Sources\n",
        "\n",
        "The system requires and manages several types of data:\n",
        "\n",
        "1. **Input Data**\n",
        "   - **User Queries**: Natural language questions and follow-ups\n",
        "   - **Conversation History**: Previous interactions for context\n",
        "   - **User Preferences**: Optional settings for search behavior\n",
        "\n",
        "2. **Knowledge Sources**\n",
        "   - **ArXiv Papers**:\n",
        "     - Source: ArXiv API\n",
        "     - Format: PDF documents\n",
        "     - Update Frequency: Daily\n",
        "     - Coverage: Scientific and technical papers\n",
        "   \n",
        "   - **Web Content**:\n",
        "     - Source: Tavily API\n",
        "     - Format: Web pages and documents\n",
        "     - Update Frequency: Real-time\n",
        "     - Coverage: News, blogs, documentation, etc.\n",
        "\n",
        "3. **Processed Data**\n",
        "   - **Embeddings**: Vector representations of text\n",
        "     - Generated using OpenAI's embedding model\n",
        "     - Stored in vector database\n",
        "   \n",
        "   - **Chunks**: Processed text segments\n",
        "     - Size: Optimized for semantic search\n",
        "     - Metadata: Source, date, relevance score\n",
        "   \n",
        "   - **Citations**: Reference information\n",
        "     - Paper titles, authors, URLs\n",
        "     - Web page sources and dates\n",
        "\n",
        "4. **Output Data**\n",
        "   - **Responses**: Generated answers with citations\n",
        "   - **Search Results**: Ranked and filtered information\n",
        "   - **Conversation Logs**: Interaction history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0zyWACuhcfqG",
        "outputId": "0048b093-2a1c-4ee0-fb52-dacd7acb5268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# %% [code]\n",
        "# Install required packages\n",
        "! pip install -qU langchain langgraph pypdf chromadb tavily-python openai python-dotenv pyboxen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ai9xIIrecwKb",
        "outputId": "7a424235-4e9b-4a7e-c6e0-d7b68a840b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.51)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.30)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.74.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.28)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting numpy<3,>=1.26.2 (from langchain-community)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.34.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.24.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (32.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.16)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.45.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: numpy, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_openai, langchain-community, chromadb, langchain_chroma\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 1.0.4\n",
            "    Uninstalling chromadb-1.0.4:\n",
            "      Successfully uninstalled chromadb-1.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chromadb-0.6.3 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 langchain_chroma-0.2.2 langchain_openai-0.3.12 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-1.26.4 pydantic-settings-2.8.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain-community langchain_chroma langchain_core langchain_openai langchain_text_splitters langgraph tavily-python openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDJAvAUpcjmN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Import required libraries\n",
        "import os  # Provides functions to interact with the operating system.\n",
        "from pyboxen import boxen  # Used to display stylized boxes in the terminal for better CLI UI.\n",
        "from getpass import getpass  # Allows secure password input without echoing.\n",
        "from typing import TypedDict, List, Dict, Optional, Literal, Union, Annotated, cast  # Used for type annotations and static type checking.\n",
        "from langchain_core.documents import Document  # Represents and structures text data in LangChain.\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses raw LLM output into usable string format.\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Loads and extracts text from PDF documents.\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter  # Splits text into chunks using markdown headers or character limits.\n",
        "from langchain_chroma import Chroma  # Provides integration with Chroma vector store for embedding storage and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Interfaces with OpenAI for embeddings and chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # Manages prompt templates for chat-based interactions.\n",
        "from langgraph.graph import StateGraph, END  # Helps define state-based logic flows for chat systems.\n",
        "from tavily import TavilyClient  # Interfaces with Tavily for real-time web search.\n",
        "from langchain.memory import ConversationBufferMemory  # Maintains memory of past conversation for context retention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOl5svrxoDkQ"
      },
      "source": [
        "# API Key Submission\n",
        "\n",
        "Please follow the instructions below:\n",
        "\n",
        "1. **Provide the Tavily API Key**\n",
        "2. **Provide the Open API Key**\n",
        "3. **Press Enter** to proceed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1CrD7yecpRF",
        "outputId": "9de0939e-aaea-4f0e-f75f-c120b60ecaa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily API Key (get from https://app.tavily.com): 路路路路路路路路路路\n",
            "Enter OpenAI API Key: 路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "# Set API keys\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter Tavily API Key (get from https://app.tavily.com): \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsoCt4BhfRp"
      },
      "source": [
        "## 2. Define State and System Architecture\n",
        "\n",
        "We'll define our system's state and flow using **LangGraph**. The state will track our:\n",
        "- **Input question**\n",
        "- **Retrieved ArXiv results**\n",
        "- **Web search results**\n",
        "- **Final answer**\n",
        "- **Conversation history for context**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DX0VBugddBN"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Define our system state - this is what passes between nodes in our graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State definition for our agentic RAG system\"\"\"\n",
        "    question: str  # User's current question\n",
        "    arxiv_results: Optional[List[Document]]  # Results from ArXiv papers (if any)\n",
        "    web_results: Optional[List[Dict]]  # Results from web search (if any)\n",
        "    answer: str  # Final synthesized answer\n",
        "    conversation_history: str  # Previous Q&A for context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym7UTPc4hpli"
      },
      "source": [
        "## 3. Router Node Implementation\n",
        "\n",
        "The **Router Node** is responsible for deciding whether to use **ArXiv papers** or **web search**.\n",
        "- **First**, it tries to use **ArXiv papers** (our local knowledge source).\n",
        "- **Falls back** to **web search** if needed.\n",
        "\n",
        "This demonstrates **strategic decision-making capabilities**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oSbkgqwdh_R"
      },
      "outputs": [],
      "source": [
        "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a highly specialized research assistant with access to two information sources:\n",
        "1. A collection of ArXiv research papers\n",
        "2. A web search tool\n",
        "\n",
        "Your task is to determine which source would be better to answer the user's question.\n",
        "FIRST try to use ArXiv papers for scientific and academic questions.\n",
        "ONLY use web search if:\n",
        "- The question requires very recent information not likely in research papers\n",
        "- The question is about general knowledge, news, or non-academic topics\n",
        "- The question asks for information beyond what academic papers would contain\n",
        "\n",
        "Consider the conversation history for context.\n",
        "\n",
        "Question: {question}\n",
        "Conversation History: {conversation_history}\n",
        "\n",
        "Respond with ONLY ONE of these two options:\n",
        "\"arxiv\" - if the question should be answered using research papers\n",
        "\"web\" - if the question requires web search\n",
        "\n",
        "Your decision should be a single word only (either \"arxiv\" or \"web\"). Do not include any explanation, reasoning, or additional text in your response.\n",
        "\"\"\")\n",
        "\n",
        "def router_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Determines whether to use ArXiv papers or web search based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question and conversation history\n",
        "\n",
        "    Returns:\n",
        "        Dict indicating which path to take next\n",
        "    \"\"\"\n",
        "    # Use a lighter model for routing decisions\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "    # Create a chain that outputs just the decision text\n",
        "    chain = router_prompt | llm\n",
        "\n",
        "    # Invoke the chain with our question and history\n",
        "    # Get the content of the AIMessage object instead of directly calling strip()\n",
        "    decision = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    }).content.strip().lower()\n",
        "\n",
        "    print(f\"Router decision: {decision}\")\n",
        "\n",
        "    # Return the next node to be called based on the decision\n",
        "    if \"web\" in decision:\n",
        "        return {\"next\": \"web_search\"}\n",
        "    else:\n",
        "        return {\"next\": \"arxiv_retrieval\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D_WGWpKhz4D"
      },
      "source": [
        "# ArXiv Processor Documentation\n",
        "\n",
        "## Overview\n",
        "The `ArXivProcessor` class is designed to handle processing ArXiv PDFs for retrieval-augmented generation (RAG) systems. It implements document-aware chunking strategies specifically optimized for scientific papers.\n",
        "\n",
        "## Key Features\n",
        "- **Two-step chunking strategy**:\n",
        " 1. Markdown header splitting to preserve document structure\n",
        " 2. Recursive character splitting for handling longer sections effectively\n",
        "- **Confidence-based retrieval** with threshold filtering\n",
        "- **Metadata preservation** from original PDFs\n",
        "\n",
        "## Class Structure\n",
        "\n",
        "### Constructor: `__init__()`\n",
        "Initializes the processor with specialized document chunking strategies:\n",
        "- `MarkdownHeaderTextSplitter` to maintain document section structure\n",
        "- `RecursiveCharacterTextSplitter` for detailed content subdivision\n",
        "\n",
        "### Methods\n",
        "\n",
        "#### `load_and_process(pdf_urls: List[str])`\n",
        "Processes ArXiv PDFs with document-aware chunking:\n",
        "- Loads PDFs from provided URLs\n",
        "- Converts content to markdown-style text with headers\n",
        "- Applies two-stage chunking process\n",
        "- Creates a vector store with OpenAI embeddings\n",
        "\n",
        "#### `retrieve(question: str, confidence_threshold: float = 0.75, k: int = 5)`\n",
        "Retrieves relevant chunks with confidence scoring:\n",
        "- Performs similarity search based on user query\n",
        "- Filters results by confidence threshold\n",
        "- Returns only high-relevance document chunks\n",
        "\n",
        "## Implementation Example\n",
        "The documented code includes a sample implementation that loads and processes two ArXiv papers:\n",
        "- Quantum computing paper: https://arxiv.org/pdf/2305.10343.pdf\n",
        "- LLM research paper: https://arxiv.org/pdf/2303.04137.pdf\n",
        "\n",
        "## Dependencies\n",
        "- `PyPDFLoader` for PDF handling\n",
        "- `MarkdownHeaderTextSplitter` and `RecursiveCharacterTextSplitter` for content chunking\n",
        "- `OpenAIEmbeddings` for vector embeddings\n",
        "- `Chroma` for vector storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "QeTPjOKgdom7",
        "outputId": "58d44f77-25e9-40c6-ae75-b6a5f976ab6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m\u001b[0m\u001b[36m >>> Initialization \u001b[0m\u001b[36m\u001b[0m\u001b[36m\u001b[0m                                                         \n",
            "\u001b[36m\u001b[0m                                                        \u001b[36m\u001b[0m                                                         \n",
            "\u001b[36m\u001b[0m   Initializing ArXiv processor with sample papers...   \u001b[36m\u001b[0m                                                         \n",
            "\u001b[36m\u001b[0m                                                        \u001b[36m\u001b[0m                                                         \n",
            "\u001b[36m扳\u001b[0m                                                         \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[0m\u001b[34m >>> PDF Loading \u001b[0m\u001b[34m\u001b[0m\u001b[34m\u001b[0m                                                          \n",
            "\u001b[34m\u001b[0m                                                       \u001b[34m\u001b[0m                                                          \n",
            "\u001b[34m\u001b[0m   Loading PDF from https://arxiv.org/pdf/2504.10412   \u001b[34m\u001b[0m                                                          \n",
            "\u001b[34m\u001b[0m                                                       \u001b[34m\u001b[0m                                                          \n",
            "\u001b[34m扳\u001b[0m                                                          \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[0m\u001b[32m >>> Processing Complete \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m                                                                              \n",
            "\u001b[32m\u001b[0m                                   \u001b[32m\u001b[0m                                                                              \n",
            "\u001b[32m\u001b[0m   Created 35 chunks from 1 PDFs   \u001b[32m\u001b[0m                                                                              \n",
            "\u001b[32m\u001b[0m                                   \u001b[32m\u001b[0m                                                                              \n",
            "\u001b[32m扳\u001b[0m                                                                              \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[0m\u001b[32m >>> Status \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m                                                                               \n",
            "\u001b[32m\u001b[0m                                  \u001b[32m\u001b[0m                                                                               \n",
            "\u001b[32m\u001b[0m   ArXiv processor initialized!   \u001b[32m\u001b[0m                                                                               \n",
            "\u001b[32m\u001b[0m                                  \u001b[32m\u001b[0m                                                                               \n",
            "\u001b[32m扳\u001b[0m                                                                               \n",
            "\n"
          ]
        }
      ],
      "source": [
        "class ArXivProcessor:\n",
        "    \"\"\"\n",
        "    Handles processing ArXiv PDFs for retrieval-augmented generation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the processor with document-aware chunking strategies.\n",
        "\n",
        "        The chunking strategy uses a two-step approach:\n",
        "        1. Markdown header splitting preserves document structure and headers\n",
        "        2. Recursive character splitting handles longer sections effectively\n",
        "        \"\"\"\n",
        "        # Header splitter preserves section structure in scientific papers\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"Section\"),           # Main sections\n",
        "                (\"##\", \"Subsection\"),       # Subsections\n",
        "                (\"###\", \"Subsubsection\")    # Sub-subsections\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Recursive splitter handles nested hierarchies and technical content\n",
        "        # - Chunk size of 1000 balances context vs specificity\n",
        "        # - Overlap of 200 ensures continuity between chunks\n",
        "        # - Separators prioritize natural breaks in scientific text\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # Will be initialized when documents are loaded\n",
        "        self.vector_store = None\n",
        "\n",
        "    def load_and_process(self, pdf_urls: List[str]):\n",
        "        \"\"\"\n",
        "        Process ArXiv PDFs with document-aware chunking\n",
        "\n",
        "        Args:\n",
        "            pdf_urls: List of URLs to ArXiv PDFs\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        # Process each PDF\n",
        "        for url in pdf_urls:\n",
        "            print(boxen(f\"Loading PDF from {url}\", title=\">>> PDF Loading\", color=\"blue\", padding=1))\n",
        "            loader = PyPDFLoader(url)\n",
        "            pages = loader.load()\n",
        "\n",
        "            # Process each page\n",
        "            for page in pages:\n",
        "                # Convert PDF content to markdown-style text with headers\n",
        "                page_text = f\"# {page.metadata['source']}\\n## Page {page.metadata['page']}\\n{page.page_content}\"\n",
        "\n",
        "                # First split by headers to maintain document structure\n",
        "                header_chunks = self.header_splitter.split_text(page_text)\n",
        "\n",
        "                # Then split large sections into smaller chunks\n",
        "                small_chunks = self.text_splitter.split_documents(header_chunks)\n",
        "\n",
        "                # Add to our collection\n",
        "                all_chunks.extend(small_chunks)\n",
        "\n",
        "        print(boxen(f\"Created {len(all_chunks)} chunks from {len(pdf_urls)} PDFs\", title=\">>> Processing Complete\", color=\"green\", padding=1))\n",
        "\n",
        "        # Create vector store with OpenAI embeddings\n",
        "        self.vector_store = Chroma.from_documents(\n",
        "            documents=all_chunks,\n",
        "            embedding=OpenAIEmbeddings(),\n",
        "            persist_directory=\"./arxiv_db\"\n",
        "        )\n",
        "\n",
        "    def retrieve(self, question: str, confidence_threshold: float = 0.75, k: int = 5):\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with confidence scoring\n",
        "\n",
        "        Args:\n",
        "            question: User question to find relevant information for\n",
        "            confidence_threshold: Minimum relevance score (0-1) to include a result\n",
        "            k: Maximum number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant document chunks that meet the threshold\n",
        "        \"\"\"\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"No ArXiv documents loaded. Run load_and_process first.\")\n",
        "\n",
        "        # Perform similarity search with relevance scores\n",
        "        results = self.vector_store.similarity_search_with_relevance_scores(\n",
        "            question, k=k\n",
        "        )\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        filtered_results = [doc for doc, score in results if score >= confidence_threshold]\n",
        "\n",
        "        print(boxen(f\"Found {len(filtered_results)} relevant chunks above threshold {confidence_threshold}\", title=\">>> Context\", color=\"yellow\", padding=1))\n",
        "\n",
        "        return filtered_results\n",
        "\n",
        "# Load sample ArXiv PDFs\n",
        "print(boxen(\"Initializing ArXiv processor with sample papers...\", title=\">>> Initialization\", color=\"cyan\", padding=1))\n",
        "arxiv_processor = ArXivProcessor()\n",
        "arxiv_processor.load_and_process([\n",
        "    \"https://arxiv.org/pdf/2504.10412\"   # LLM research paper\n",
        "])\n",
        "print(boxen(\"ArXiv processor initialized!\", title=\">>> Status\", color=\"green\", padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeaD77BupxMv"
      },
      "source": [
        "# ArXiv Retrieval Node Documentation\n",
        "\n",
        "## Overview\n",
        "The `arxiv_retrieval_node` function serves as a retrieval component in an agent-based system, fetching relevant scientific information from ArXiv papers based on user queries.\n",
        "\n",
        "## Function Signature\n",
        "`arxiv_retrieval_node(state: AgentState) -> dict`\n",
        "\n",
        "## Parameters\n",
        "- `state`: An AgentState object containing the current conversation state, including:\n",
        " - `question`: The user's query to search for in ArXiv papers\n",
        "\n",
        "## Functionality\n",
        "The function:\n",
        "1. Extracts the user's question from the input state\n",
        "2. Calls the `arxiv_processor.retrieve()` method to find relevant document chunks\n",
        "3. Uses a reduced confidence threshold (0.5) compared to the default (0.75) to improve recall\n",
        "4. Returns the retrieved documents for further processing\n",
        "\n",
        "## Return Value\n",
        "Returns a dictionary with:\n",
        "- `arxiv_results`: A list of document chunks from ArXiv papers relevant to the user's question\n",
        "\n",
        "## Integration Notes\n",
        "- This function is designed to be used as a node in an agent workflow\n",
        "- The reduced confidence threshold ensures more potential matches are returned, prioritizing recall over precision\n",
        "- The retrieved documents can be used by subsequent nodes for answering the user's question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QI0RXSbdvCv"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def arxiv_retrieval_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant information from ArXiv papers based on the question.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with arxiv_results\n",
        "    \"\"\"\n",
        "    # Retrieve relevant documents from ArXiv\n",
        "    relevant_docs = arxiv_processor.retrieve(\n",
        "        question=state[\"question\"],\n",
        "        confidence_threshold=0.5  # Adjusted threshold for better recall\n",
        "    )\n",
        "\n",
        "    # Check if we found enough relevant content\n",
        "    return {\"arxiv_results\": relevant_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SqO1XREiD7z"
      },
      "source": [
        "## 5. Web Search Node Implementation\n",
        "\n",
        "The **Web Search Node** uses the **Tavily API** to search the web when **ArXiv papers** don't have the answer.\n",
        "\n",
        "- **Optimizes** the search query\n",
        "- **Filters and processes** results\n",
        "- **Ensures** proper attribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15KqqPGedzln"
      },
      "outputs": [],
      "source": [
        "def synthesize_answer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Synthesizes a comprehensive answer from retrieved information.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing question and retrieved information\n",
        "\n",
        "    Returns:\n",
        "        Updated state with answer\n",
        "    \"\"\"\n",
        "    # Determine which source to use for synthesis\n",
        "    if state[\"arxiv_results\"] and len(state[\"arxiv_results\"]) > 0:\n",
        "        # Using ArXiv research papers\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Document: {d.metadata.get('source', 'Unknown')} (Page {d.metadata.get('page', 'Unknown')}) ---\\n{d.page_content}\"\n",
        "            for d in state[\"arxiv_results\"]\n",
        "        ])\n",
        "\n",
        "        prompt_template =\"\"\"\n",
        "        You are a knowledgeable research assistant specializing in mathematical theory and scientific literature analysis.\n",
        "\n",
        "        Your goal is to generate clean, formatted responses to user questions based solely on the provided ArXiv sources.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Relevant Extracts from ArXiv Papers:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        ---\n",
        "\n",
        "        Instructions for Synthesizing the Answer:\n",
        "\n",
        "        1. Read the extracts thoroughly and understand the concepts.\n",
        "        2. Answer the question comprehensively using only the provided context.\n",
        "        3. Organize the response into the following markdown sections (if applicable):\n",
        "          - Summary\n",
        "          - Key Concepts\n",
        "          - Theoretical Results\n",
        "          - Implications / Applications\n",
        "        4. Cite from the paper in the format: (Author et al., Page X). If page number is unknown, write: (Author et al.).\n",
        "        6. Avoid repetition, excessive formal tone, or generic commentary. Be clear and concise.**\n",
        "        7. If the provided text lacks enough detail to answer, state it clearly and suggest what additional info is needed.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Now, write a well-structured, markdown-formatted answer to the question and it should be in a readable format as well.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Using web search results\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Source {i+1}: {res['title']} ---\\n{res['content']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        prompt_template = \"\"\"\n",
        "        You are a knowledgeable research assistant providing accurate information based on web search results.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Here are relevant web search results:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        Instructions:\n",
        "        1. Synthesize a comprehensive answer using ONLY the information provided above.\n",
        "        2. Cite sources using [1], [2], etc. corresponding to the source numbers above.\n",
        "        3. If the search results don't contain sufficient information, acknowledge the limitations.\n",
        "        4. DO NOT make up information not present in the sources.\n",
        "        5. Include only facts supported by the sources.\n",
        "\n",
        "        Your answer:\n",
        "        \"\"\"\n",
        "\n",
        "    # Create the prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    # Use a more capable model for synthesis\n",
        "    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "    chain = synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Generate the answer\n",
        "    response = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"sources\": sources,\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    })\n",
        "\n",
        "    # Add source citations for web results\n",
        "    if state.get(\"web_results\") and not state.get(\"arxiv_results\"):\n",
        "        answer_content = response\n",
        "\n",
        "        # Add URL references at the end\n",
        "        url_citations = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "            f\"[{i+1}] {res['url']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        answer_content += url_citations\n",
        "    else:\n",
        "        answer_content = response\n",
        "\n",
        "    # Return just the answer content without additional formatting\n",
        "    return {\"answer\": answer_content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WsFmnmpd3CI"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "def web_search_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Searches the web for information using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing the question\n",
        "\n",
        "    Returns:\n",
        "        Updated state with web_results\n",
        "    \"\"\"\n",
        "    # Include academic domains to improve search quality\n",
        "    academic_domains = [\"arxiv.org\", \"scholar.google.com\", \"researchgate.net\", \"edu\"]\n",
        "\n",
        "    # Get search results\n",
        "    results = web_searcher.search(\n",
        "        query=state[\"question\"],\n",
        "        max_results=5,\n",
        "        include_domains=academic_domains\n",
        "    )\n",
        "\n",
        "    return {\"web_results\": results}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoo-tCciKHn"
      },
      "source": [
        "# Function Documentation: `synthesize_answer_node`\n",
        "\n",
        "## Overview\n",
        "The `synthesize_answer_node` function is a key component in a LangChain-based conversational agent. It is responsible for generating a comprehensive answer based on either scientific research papers (from ArXiv) or web search results (via Tavily). The generated response is contextual, well-structured, and strictly grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To synthesize a high-quality, structured, and citation-backed answer from the information retrieved during the conversational flow  either from ArXiv research papers or real-time web search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- **state (AgentState)**:  \n",
        "  A dictionary representing the current state of the agent, which includes:\n",
        "  - `question`: The user's query.\n",
        "  - `arxiv_results`: A list of research paper excerpts (if available).\n",
        "  - `web_results`: A list of web search results (used when no ArXiv data is present).\n",
        "  - `conversation_history`: Context from previous exchanges to maintain continuity.\n",
        "\n",
        "---\n",
        "\n",
        "## Logic Flow\n",
        "\n",
        "1. **Source Determination**:  \n",
        "   The function first checks whether ArXiv results are available. If so, it uses them; otherwise, it falls back to web search results.\n",
        "\n",
        "2. **Prompt Construction**:  \n",
        "   A custom `prompt_template` is built depending on the data source. Each template includes:\n",
        "   - The original question.\n",
        "   - Retrieved content (formatted accordingly).\n",
        "   - Prior conversation context.\n",
        "   - Explicit instructions to ensure grounded, factual, and well-structured responses.\n",
        "\n",
        "3. **Model Invocation**:  \n",
        "   - Uses `ChatOpenAI` (specifically `gpt-4-turbo`) for advanced reasoning and response generation.\n",
        "   - Combines the prompt and model into a LangChain chain using `ChatPromptTemplate` and `StrOutputParser`.\n",
        "\n",
        "4. **Response Handling**:  \n",
        "   - If web results were used, the function appends a list of source URLs at the end of the response.\n",
        "   - If ArXiv sources were used, inline citations in the format `(Author et al., Page X)` are expected.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "- Returns a dictionary with a single key:  \n",
        "  - `answer`: A fully formatted, cited response derived from either research papers or search results.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Characteristics\n",
        "\n",
        "- **Grounded Output**: The model is instructed not to hallucinate or invent facts.\n",
        "- **Citations Included**: Adds credibility and traceability via inline citations or URL references.\n",
        "- **Context-Aware**: Maintains conversation context to provide coherent multi-turn interactions.\n",
        "- **Readable Format**: Uses markdown elements such as headers, bullet points, and bold text for readability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h2iGpE9d8VM"
      },
      "outputs": [],
      "source": [
        "def synthesize_answer_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Synthesizes a comprehensive answer from retrieved information.\n",
        "\n",
        "    Args:\n",
        "        state: Current state containing question and retrieved information\n",
        "\n",
        "    Returns:\n",
        "        Updated state with answer\n",
        "    \"\"\"\n",
        "    # Determine which source to use for synthesis\n",
        "    if state[\"arxiv_results\"] and len(state[\"arxiv_results\"]) > 0:\n",
        "        # Using ArXiv research papers\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Document: {d.metadata.get('source', 'Unknown')} (Page {d.metadata.get('page', 'Unknown')}) ---\\n{d.page_content}\"\n",
        "            for d in state[\"arxiv_results\"]\n",
        "        ])\n",
        "        displayed_sources = sources\n",
        "        source_type = \"ArXiv Papers\"\n",
        "\n",
        "        prompt_template =\"\"\"\n",
        "        You are a knowledgeable research assistant specializing in mathematical theory and scientific literature analysis.\n",
        "\n",
        "        Your goal is to generate clean, formatted responses to user questions based solely on the provided ArXiv sources.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Relevant Extracts from ArXiv Papers:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        ---\n",
        "\n",
        "        Instructions for Synthesizing the Answer:\n",
        "\n",
        "        1. Read the extracts thoroughly and understand the concepts.\n",
        "        2. Answer the question comprehensively using only the provided context.\n",
        "        3. Organize the response into the following markdown sections (if applicable):\n",
        "          - Summary\n",
        "          - Key Concepts\n",
        "          - Theoretical Results\n",
        "          - Implications / Applications\n",
        "        4. Cite from the paper in the format: (Author et al., Page X). If page number is unknown, write: (Author et al.).\n",
        "        6. Avoid repetition, excessive formal tone, or generic commentary. Be clear and concise.**\n",
        "        7. If the provided text lacks enough detail to answer, state it clearly and suggest what additional info is needed.\n",
        "\n",
        "        ---\n",
        "\n",
        "        Now, write a well-structured, markdown-formatted answer to the question and it should be in a readable format as well.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Using web search results\n",
        "        sources = \"\\n\\n\".join([\n",
        "            f\"--- Source {i+1}: {res['title']} ---\\n{res['content']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "        displayed_sources = sources\n",
        "        source_type = \"Web Search Results\"\n",
        "\n",
        "        prompt_template = \"\"\"\n",
        "        You are a knowledgeable research assistant providing accurate information based on web search results.\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Here are relevant web search results:\n",
        "        {sources}\n",
        "\n",
        "        Conversation History:\n",
        "        {conversation_history}\n",
        "\n",
        "        Instructions:\n",
        "        1. Synthesize a comprehensive answer using ONLY the information provided above.\n",
        "        2. Cite sources using [1], [2], etc. corresponding to the source numbers above.\n",
        "        3. If the search results don't contain sufficient information, acknowledge the limitations.\n",
        "        4. DO NOT make up information not present in the sources.\n",
        "        5. Include only facts supported by the sources.\n",
        "\n",
        "        Your answer:\n",
        "        \"\"\"\n",
        "\n",
        "    # Print retrieval information\n",
        "    print(f\"\\n=== Retrieved chunks from {source_type} ===\")\n",
        "    print(displayed_sources)\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create the prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    # Use a more capable model for synthesis\n",
        "    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "    chain = synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Generate the answer\n",
        "    response = chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"sources\": sources,\n",
        "        \"conversation_history\": state[\"conversation_history\"]\n",
        "    })\n",
        "\n",
        "    # Add source citations for web results\n",
        "    if state.get(\"web_results\") and not state.get(\"arxiv_results\"):\n",
        "        answer_content = response\n",
        "\n",
        "        # Add URL references at the end\n",
        "        url_citations = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "            f\"[{i+1}] {res['url']}\"\n",
        "            for i, res in enumerate(state[\"web_results\"] or [])\n",
        "        ])\n",
        "\n",
        "        answer_content += url_citations\n",
        "    else:\n",
        "        answer_content = response\n",
        "\n",
        "    # Using markdown and plain text for better readability\n",
        "    formatted_output = f\"\"\"\n",
        "## Context\n",
        "**Question:** {state[\"question\"]}\n",
        "**Source:** {source_type}\n",
        "\n",
        "## Response\n",
        "{answer_content}\n",
        "\"\"\"\n",
        "\n",
        "    return {\"answer\": formatted_output}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePSZSMQBiaMC"
      },
      "source": [
        "## 7. Conversation Memory Node\n",
        "\n",
        "This node **manages conversation history** to provide context for **multi-turn interactions**.\n",
        "\n",
        "- **Stores** previous Q&A\n",
        "- **Updates** the state with the current interaction\n",
        "- **Maintains** a sliding window of relevant history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgefbaa_eAfZ"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Initialize conversation memory\n",
        "memory = ConversationBufferMemory(return_messages=False, output_key=\"answer\", input_key=\"question\")\n",
        "\n",
        "def update_memory_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Updates the conversation memory with the current Q&A pair.\n",
        "\n",
        "    Args:\n",
        "        state: Current state with question and answer\n",
        "\n",
        "    Returns:\n",
        "        Updated state with new conversation_history\n",
        "    \"\"\"\n",
        "    # Save the current interaction to memory\n",
        "    memory.save_context(\n",
        "        {\"question\": state[\"question\"]},\n",
        "        {\"answer\": state[\"answer\"]}\n",
        "    )\n",
        "\n",
        "    # Get the updated conversation history\n",
        "    updated_history = memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Return the updated state\n",
        "    return {\"conversation_history\": updated_history}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxKEeu64iglN"
      },
      "source": [
        "# Workflow State Graph Setup\n",
        "\n",
        "## Overview\n",
        "This section sets up the **LangGraph state machine** for managing the conversational agents workflow. It defines how user queries are processed step-by-step using modular nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To create a graph-based control flow that determines how the agent processes input, performs retrieval, synthesizes responses, updates memory, and eventually ends the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **Workflow Initialization**\n",
        "- A new `StateGraph` is initialized with the `AgentState` type, defining the structure of the workflow.\n",
        "\n",
        "### 2. **Node Definitions**\n",
        "The graph is composed of several functional nodes, each responsible for a specific task:\n",
        "- **router**: Determines whether to fetch data from the web or ArXiv.\n",
        "- **arxiv_retrieval**: Retrieves relevant research papers from ArXiv.\n",
        "- **web_search**: Retrieves web results via Tavily.\n",
        "- **synthesize**: Synthesizes a final answer from the retrieved information.\n",
        "- **update_memory**: Stores the interaction context for future turns.\n",
        "\n",
        "### 3. **Entry Point**\n",
        "- The `router` node is set as the initial entry point for the graph, meaning every workflow starts with routing logic.\n",
        "\n",
        "### 4. **Conditional Routing**\n",
        "- A conditional edge is established from `router` based on the `\"next\"` field in the state:\n",
        "  - If `\"next\"` is `\"web_search\"`, it routes to the `web_search` node.\n",
        "  - If `\"next\"` is `\"arxiv_retrieval\"`, it routes to the `arxiv_retrieval` node.\n",
        "\n",
        "### 5. **Workflow Sequence**\n",
        "The following fixed transitions define the remainder of the workflow:\n",
        "- From either `web_search` or `arxiv_retrieval`  go to `synthesize`\n",
        "- From `synthesize`  go to `update_memory`\n",
        "- From `update_memory`  reach `END` (completion of the flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRYbjFhHeEZt"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Create the workflow state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes to the graph\n",
        "workflow.add_node(\"router\", router_node)\n",
        "workflow.add_node(\"arxiv_retrieval\", arxiv_retrieval_node)\n",
        "workflow.add_node(\"web_search\", web_search_node)\n",
        "workflow.add_node(\"synthesize\", synthesize_answer_node)\n",
        "workflow.add_node(\"update_memory\", update_memory_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# Define conditional edges from router\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state[\"next\"],\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"arxiv_retrieval\": \"arxiv_retrieval\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define rest of the edges\n",
        "workflow.add_edge(\"arxiv_retrieval\", \"synthesize\")\n",
        "workflow.add_edge(\"web_search\", \"synthesize\")\n",
        "workflow.add_edge(\"synthesize\", \"update_memory\")\n",
        "workflow.add_edge(\"update_memory\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4TP3xSioHc"
      },
      "source": [
        "## 9. Testing the System\n",
        "\n",
        "Let's **test our system** with different types of questions:\n",
        "\n",
        "- **Questions answerable** from ArXiv papers\n",
        "- **Questions requiring** web search\n",
        "- **Follow-up questions** to test memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gFoxsXLWeHSv"
      },
      "outputs": [],
      "source": [
        "def ask(question: str):\n",
        "    \"\"\"\n",
        "    Ask a question to the agentic RAG system.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "\n",
        "    Returns:\n",
        "        The system's answer\n",
        "    \"\"\"\n",
        "    # Initialize the state\n",
        "    initial_state = {\n",
        "        \"question\": question,\n",
        "        \"arxiv_results\": None,\n",
        "        \"web_results\": None,\n",
        "        \"answer\": \"\",\n",
        "        \"conversation_history\": memory.load_memory_variables({}).get(\"history\", \"\")\n",
        "    }\n",
        "\n",
        "    # Invoke the workflow\n",
        "    result = app.invoke(initial_state)\n",
        "    # Print the response details with pyboxen\n",
        "    print(boxen(f\"Question: {result['question']}\", title=\">>> Question\", color=\"blue\", padding=1))\n",
        "\n",
        "    if result[\"arxiv_results\"]:\n",
        "        arxiv_count = len(result[\"arxiv_results\"])\n",
        "        print(boxen(f\"Found {arxiv_count} ArXiv results\", title=\">>> ArXiv Results\", color=\"magenta\", padding=1))\n",
        "    elif result[\"web_results\"]:\n",
        "        web_count = len(result[\"web_results\"])\n",
        "        print(boxen(f\"Found {web_count} Web results\", title=\">>> Web Results\", color=\"magenta\", padding=1))\n",
        "    else:\n",
        "        print(boxen(\"No results found\", title=\">>> Results\", color=\"red\", padding=1))\n",
        "\n",
        "    print(boxen(result[\"answer\"], title=\">>> Answer\", color=\"green\", padding=1))\n",
        "\n",
        "\n",
        "    # # Print the response directly without additional formatting\n",
        "    # print(result[\"question\"])\n",
        "    # print(result[\"arxiv_results\"])\n",
        "    # print(result[\"answer\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0Ps7Nw1exMl",
        "outputId": "c57a4412-341b-4069-edad-5210993bd5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router decision: arxiv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\u001b[0m\u001b[33m >>> Context \u001b[0m\u001b[33m\u001b[0m\u001b[33m\u001b[0m                                                                \n",
            "\u001b[33m\u001b[0m                                                 \u001b[33m\u001b[0m                                                                \n",
            "\u001b[33m\u001b[0m   Found 5 relevant chunks above threshold 0.5   \u001b[33m\u001b[0m                                                                \n",
            "\u001b[33m\u001b[0m                                                 \u001b[33m\u001b[0m                                                                \n",
            "\u001b[33m扳\u001b[0m                                                                \n",
            "\n",
            "\n",
            "=== Retrieved chunks from ArXiv Papers ===\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineerings future.  \n",
            "Index Terms Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringthe art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highpoor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "a scalable, AI-driven path to cleaner codebases, vital for\n",
            "software engineerings future.  \n",
            "Index Terms Graph Neural Networks, Code\n",
            "Refactoring, Software Maintainability, Abstract Syntax\n",
            "Trees, Machine Learning, Cyclomatic Complexity, Code\n",
            "Coupling, Software Engineering.\n",
            "I. INTRODUCTION\n",
            "Software refactoringthe art of tweaking code to make it\n",
            "cleaner, more readable, and easier to maintain without changing\n",
            "its behaviors at the heart of modern software engineering.\n",
            "Picture a sprawling codebase: functions tangled in loops,\n",
            "variables sprawling across modules, and complexity creeping\n",
            "up like vines. Developers spend 30% of their time wrestling\n",
            "with such messes, according to a 2023 GitHub survey [1]. The\n",
            "stakes are highpoor maintainability spikes bugs by 25% and\n",
            "slows feature rollouts by 40% [2]. Traditional tools like\n",
            "SonarQube or Check style flag issues (e.g., methods with 20+\n",
            "lines), but their rigid rules miss the forest for the trees. Enter\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "peek ahead (real-time refactoring bots). Bar charts compare\n",
            "precision, tables list metrics, and AST graphs show GNN\n",
            "magic. It is a step toward codebases that do not fight back.\n",
            "II. THEORETICAL BACKGROUND\n",
            "Code refactoring is not newFowlers 1999 book codified it:\n",
            "extract methods, reduce duplication, tame complexity [8].\n",
            "Maintainability hinges on metrics: cyclomatic complexity\n",
            "(paths through code10s a red flag), coupling (module\n",
            "dependencies5+ screams trouble), and cohesion (how tight a\n",
            "modules purpose is) [9]. High complexitylike a 50-path\n",
            "functionmeans bugs hide easier; tight couplinglike twenty\n",
            "cross-module callsmeans changes ripple hard. ASTs\n",
            "formalize this. A Python line, if x > 0: y = x, becomes a tree:\n",
            "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance\n",
            "Software Maintainability\n",
            "Gopichand Bandarupalli1\n",
            "1ai.ml.research.articles@gmail.com\n",
            "1Professional M.B.A., Campbellsville university, Texas, USA\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "peek ahead (real-time refactoring bots). Bar charts compare\n",
            "precision, tables list metrics, and AST graphs show GNN\n",
            "magic. It is a step toward codebases that do not fight back.\n",
            "II. THEORETICAL BACKGROUND\n",
            "Code refactoring is not newFowlers 1999 book codified it:\n",
            "extract methods, reduce duplication, tame complexity [8].\n",
            "Maintainability hinges on metrics: cyclomatic complexity\n",
            "(paths through code10s a red flag), coupling (module\n",
            "dependencies5+ screams trouble), and cohesion (how tight a\n",
            "modules purpose is) [9]. High complexitylike a 50-path\n",
            "functionmeans bugs hide easier; tight couplinglike twenty\n",
            "cross-module callsmeans changes ripple hard. ASTs\n",
            "formalize this. A Python line, if x > 0: y = x, becomes a tree:\n",
            "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance\n",
            "Software Maintainability\n",
            "Gopichand Bandarupalli1\n",
            "1ai.ml.research.articles@gmail.com\n",
            "1Professional M.B.A., Campbellsville university, Texas, USA\n",
            "\n",
            "--- Document: Unknown (Page Unknown) ---\n",
            "complexity (aiming below 10), coupling (targeting <5\n",
            "dependencies), and refactoring precision (correct suggestions\n",
            "out of 1000). Tools like PyTorch Geometric and Tree-sitter\n",
            "parse ASTs, while PMD tracks metrics [5]. Expect deep dives\n",
            "into preprocessing (50% of files had syntax errors), GNN\n",
            "tuning (80% validation accuracy), and results (92% GNN\n",
            "precision vs. 78% SonarQube).Why care Software eats the\n",
            "world$4.5 trillion in 2025 spending [6]and maintainability is\n",
            "its lifeline. A 2022 study found 60% of developers want AI to\n",
            "automate refactoring [7]. This research delivers: a GNN\n",
            "pipeline that learns from codes bones, not just its skin,\n",
            "promising faster, smarter fixes. Sections unpack theory (ASTs,\n",
            "GNNs), past work (static vs. ML tools), methods (data\n",
            "wrangling, model specs), experiments (graphs galore), and a\n",
            "peek ahead (real-time refactoring bots). Bar charts compare\n",
            "precision, tables list metrics, and AST graphs show GNN\n",
            "magic. It is a step toward codebases that do not fight back.\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[0m\u001b[34m >>> Question \u001b[0m\u001b[34m\u001b[0m\u001b[34m\u001b[0m                                                                     \n",
            "\u001b[34m\u001b[0m                                            \u001b[34m\u001b[0m                                                                     \n",
            "\u001b[34m\u001b[0m   Question: Explain Software Refactoring   \u001b[34m\u001b[0m                                                                     \n",
            "\u001b[34m\u001b[0m                                            \u001b[34m\u001b[0m                                                                     \n",
            "\u001b[34m扳\u001b[0m                                                                     \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\u001b[0m\u001b[35m >>> ArXiv Results \u001b[0m\u001b[35m\u001b[0m\u001b[35m\u001b[0m                                                                                      \n",
            "\u001b[35m\u001b[0m                           \u001b[35m\u001b[0m                                                                                      \n",
            "\u001b[35m\u001b[0m   Found 5 ArXiv results   \u001b[35m\u001b[0m                                                                                      \n",
            "\u001b[35m\u001b[0m                           \u001b[35m\u001b[0m                                                                                      \n",
            "\u001b[35m扳\u001b[0m                                                                                      \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Context                                                                                                    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   **Question:** Explain Software Refactoring                                                                    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   **Source:** ArXiv Papers                                                                                      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Response                                                                                                   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Summary                                                                                                    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Software refactoring involves the modification of the internal structure of existing code without altering    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   its external functionality. This practice is essential for improving software attributes such as              \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   readability and maintainability. It is a fundamental aspect of modern software engineering, helping to        \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   manage code complexities and ensuring the adaptability and efficiency of codebases.                           \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Key Concepts                                                                                               \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   - **Abstract Syntax Trees (ASTs)**: These structures represent the hierarchical tree of programming           \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   language syntax, crucial for analyzing and modifying code during the refactoring process.                     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   - **Graph Neural Networks (GNNs)**: These are used to enhance refactoring by learning from ASTs, allowing     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   for more accurate recognition of refactoring opportunities.                                                   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   - **Cyclomatic Complexity**: This metric measures code complexity by counting the number of independent       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   paths through the code. Refactoring seeks to reduce this complexity.                                          \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   - **Code Coupling**: It describes the interdependencies between different modules of a program. Less          \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   coupling is preferable as it eases maintenance.                                                               \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   - **Maintainability Metrics**: These include cyclomatic complexity, coupling, and cohesion. These metrics     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   help identify segments of code that need refactoring.                                                         \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Theoretical Results                                                                                        \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Originating from principles laid down by Fowler in 1999, software refactoring emphasizes reducing             \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   duplication, extracting methods, and managing complexity. Advances in AI have furthered these principles by   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   incorporating tools like GNNs along with parsers such as tree-sitter and PyTorch Geometric to analyze ASTs    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   and maintain metrics effectively. Such advancements have resulted in notable improvements in refactoring      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   precision, with AI-driven tools achieving higher accuracy compared to traditional tools (e.g., 92%            \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   precision with GNNs versus 78% with SonarQube).                                                               \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Implications / Applications                                                                                \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   The integration of AI, particularly through GNNs and AST analysis, represents a promising advancement for     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   more precise, automatic, and efficient code restructuring. As software complexity escalates and the demand    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   for maintainable code intensifies, AI-driven refactoring tools are becoming crucial. These tools              \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   significantly reduce the time developers spend understanding and modifying codebases, thus aiding in faster   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   development cycles and enhancing software quality. Recent studies indicate a strong preference among          \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   developers for AI-based automation of refactoring tasks, underscoring its growing importance in software      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   engineering.                                                                                                  \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Software refactoring, enhanced by AI technologies, remains a vital practice in maintaining the health and     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   longevity of modern software codebases.                                                                       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m扳\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with a question about quantum computing (should use ArXiv)\n",
        "ask(\"Explain Software Refactoring\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7keGgbrWgqy9",
        "outputId": "df47a2b4-3abc-4fec-ebc4-b0d79f12b83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router decision: web\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[0m\u001b[34m >>> Web Search Query \u001b[0m\u001b[34m\u001b[0m\u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[0m                                                                                                                 \u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[0m   Searching with query: (site:arxiv.org OR site:scholar.google.com OR site:researchgate.net OR site:edu) What   \u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[0m   is Crew Ai ?                                                                                                  \u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[0m                                                                                                                 \u001b[34m\u001b[0m\n",
            "\u001b[34m扳\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\u001b[0m\u001b[33m >>> Search Results \u001b[0m\u001b[33m\u001b[0m\u001b[33m\u001b[0m                                                                                        \n",
            "\u001b[33m\u001b[0m                         \u001b[33m\u001b[0m                                                                                        \n",
            "\u001b[33m\u001b[0m   Found 5 web results   \u001b[33m\u001b[0m                                                                                        \n",
            "\u001b[33m\u001b[0m                         \u001b[33m\u001b[0m                                                                                        \n",
            "\u001b[33m扳\u001b[0m                                                                                        \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[0m\u001b[34m >>> Question \u001b[0m\u001b[34m\u001b[0m\u001b[34m\u001b[0m                                                                                \n",
            "\u001b[34m\u001b[0m                                 \u001b[34m\u001b[0m                                                                                \n",
            "\u001b[34m\u001b[0m   Question: What is Crew Ai ?   \u001b[34m\u001b[0m                                                                                \n",
            "\u001b[34m\u001b[0m                                 \u001b[34m\u001b[0m                                                                                \n",
            "\u001b[34m扳\u001b[0m                                                                                \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\u001b[0m\u001b[35m >>> Web Results \u001b[0m\u001b[35m\u001b[0m\u001b[35m\u001b[0m                                                                                        \n",
            "\u001b[35m\u001b[0m                         \u001b[35m\u001b[0m                                                                                        \n",
            "\u001b[35m\u001b[0m   Found 5 Web results   \u001b[35m\u001b[0m                                                                                        \n",
            "\u001b[35m\u001b[0m                         \u001b[35m\u001b[0m                                                                                        \n",
            "\u001b[35m扳\u001b[0m                                                                                        \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[0m\u001b[32m >>> Answer \u001b[0m\u001b[32m\u001b[0m\u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Context                                                                                                    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   **Question:** What is Crew Ai ?                                                                               \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   **Source:** Web Search Results                                                                                \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   ## Response                                                                                                   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Crew AI, often referred to as CrewAI, is described as an open-source framework that focuses on coordinating   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   AI agents within multi-agent systems. It is engineered to facilitate autonomous operations and cooperation    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   among AI agents, which are defined with specific roles, objectives, and tools [1][3]. The framework is        \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   structured around several main components: Agents, Tasks, Tools, and Crews. Agents in this context are        \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   autonomous units capable of performing tasks, making decisions, and communicating with other agents. Tasks    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   refer to the specific assignments or jobs that agents carry out, and these include executing agents and       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   required tools. Tools likely represent the resources or capabilities agents use to perform tasks, and Crews   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   denote the group or team configuration within the system [3].                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   The CrewAI framework is noted for its capability to handle complex tasks such as multi-step workflows,        \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   decision-making processes, and intricate problem-solving. It supports a range of functionalities including    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   role-customized agents, automatic task delegation, and flexible task management. To enhance its               \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   functionality, CrewAI integrates with various APIs including those from OpenAI and Ollama, and frameworks     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   like LangGraph and LangChain, ensuring a robust platform for developing multi-agent AI applications [1][3].   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   The framework is tailored to optimize team cooperation and efficiency through what is described as process    \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   orchestration, which involves precise task allocation and role definition within the crew structure. This     \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   focus suggests that CrewAI is particularly suitable for environments needing sophisticated, AI-driven         \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   solutions involving multiple collaborative agents aimed at tackling complex scenarios [1][3].                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   The information presented consistently highlights CrewAIs purpose in complex AI applications,                \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   distinguishing it from more generic uses of AI in game or crew systems, which might be mistaken from the      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   term \"crew\" used in different contexts [2].                                                                   \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Sources:                                                                                                      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [1] Source 1: Exploration of LLM Multi-Agent Application Implementation                                       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [3] Source 3: Exploration of LLM Multi-Agent Application Implementation                                       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   Sources:                                                                                                      \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [1] https://arxiv.org/html/2411.18241                                                                         \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [2] https://studio.eecs.umich.edu/confluence/display/SUM22/Crew+AI+Documentation                              \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [3] https://arxiv.org/pdf/2411.18241                                                                          \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [4] https://arxiv.org/html/2503.02068v1                                                                       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m   [5] https://arxiv.org/html/2408.00170v1                                                                       \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m\u001b[0m                                                                                                                 \u001b[32m\u001b[0m\n",
            "\u001b[32m扳\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with a question about recent developments (should use web)\n",
        "ask(\"What is Crew Ai ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TtHrxeMo1mr"
      },
      "source": [
        "##  Conclusion\n",
        "\n",
        "The Agentic RAG System with ArXiv + Web Fallback represents a powerful approach to information retrieval and synthesis, combining the best of both academic and real-time knowledge sources. By intelligently routing queries and maintaining conversation context, it provides:\n",
        "\n",
        "- **Comprehensive Answers**: Leveraging both academic papers and current web information\n",
        "- **Proper Attribution**: Ensuring all sources are properly cited\n",
        "- **Contextual Understanding**: Maintaining conversation history for coherent interactions\n",
        "- **Flexible Knowledge Access**: Adapting to different types of queries and information needs\n",
        "\n",
        "This system is particularly valuable for:\n",
        "- Researchers seeking both theoretical foundations and practical applications\n",
        "- Developers looking for up-to-date technical information\n",
        "- Students and professionals needing comprehensive, well-sourced answers\n",
        "- Anyone requiring a balance between academic rigor and current information\n",
        "\n",
        "The modular architecture and use of LangGraph make it easy to extend and adapt the system for specific use cases or additional knowledge sources."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}